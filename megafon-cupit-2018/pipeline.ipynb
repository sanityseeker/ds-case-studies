{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первичная обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(data):\n",
    "    # Не очень информативные признаки с примерно одинаковым распределением. Возьмем их сумму, а сами признаки удалим.\n",
    "    prefix = \"FEATURE_\"\n",
    "    feature_nums = [189, 187, 190, 191, 192, 193, 194]\n",
    "    features = [prefix + str(feature) for feature in feature_nums]\n",
    "    data['DROPPED_SUM'] = data[features[1:]].sum(axis=1)\n",
    "    data = data.drop(features, axis=1)\n",
    "\n",
    "\n",
    "    # Интересный признак, значения которого разбиваются на несколько групп. Сделаем бинарные признаки для каждой группы. \n",
    "    data['FEATURE_148_1'] = data.FEATURE_148 < 0.2\n",
    "    data['FEATURE_148_2'] = ((data.FEATURE_148 < 0.4) & (data.FEATURE_148 > 0.2)).astype(float)\n",
    "    data['FEATURE_148_3'] = ((data.FEATURE_148 < 0.6) & (data.FEATURE_148 > 0.4)).astype(float)\n",
    "    data['FEATURE_148_4'] = ((data.FEATURE_148 < 0.8) & (data.FEATURE_148 > 0.6)).astype(float)\n",
    "    data['FEATURE_148_5'] = ((data.FEATURE_148 < 1.0) & (data.FEATURE_148 > 0.8)).astype(float)\n",
    "\n",
    "    # Признаки с явно выраженными модами. Построим бинарные признаки, указывающие, к какому пику относится значение.\n",
    "    to_categories_nums = [(127, 95), (135, 95), (183, 1400), (77, 20), (87, 4), (88, 5), (186, 2500), (244, 40)]\n",
    "    for index, value in to_categories_nums:\n",
    "        data[prefix + str(index) + '_1'] = (data[prefix + str(index)] > value).astype(float) \n",
    "\n",
    "    # Признаки, которые суммируются в другой признак, но не всегда, поэтому добавим отличаются ли суммы как отдельный признак.\n",
    "    column_names = []\n",
    "    prefix = 'FEATURE_'\n",
    "    for i in range(149, 154):\n",
    "        column_names.append(prefix + str(i))\n",
    "\n",
    "    df149_first = data[column_names[:-1]]\n",
    "    df153 = data[column_names[-1:]]\n",
    "    data['SUM_FEATURE'] = ((pd.Series(df153.values.flatten()) - df149_first.sum(axis=1)) > 0).astype('float')\n",
    "    return data\n",
    "\n",
    "def preprocess_train_data(data):\n",
    "    target = data['TARGET']\n",
    "    data = data.drop(labels=['TARGET', 'ID'], axis=1)\n",
    "\n",
    "    data = add_features(data)\n",
    "\n",
    "    # Заполним пропуски, отшкалируем значения и разобьем на обучающую и валидационную выборки.\n",
    "    clean_data = data.fillna(data.median(axis=0))\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    clean_data[clean_data.columns] = scaler.fit_transform(clean_data[clean_data.columns])\n",
    "    return clean_data, target, scaler\n",
    "\n",
    "def preprocess_test_data(data, scaler):\n",
    "    ID = data['ID']\n",
    "    data = data.drop(labels=['ID'], axis=1)\n",
    "    data = add_features(data)\n",
    "    clean_data = data.fillna(data.median(axis=0))\n",
    "    clean_data[clean_data.columns] = scaler.fit_transform(clean_data[clean_data.columns])\n",
    "    return clean_data, ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/contest_train.csv')\n",
    "clean_data, target, scaler = preprocess_train_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEATURE_0</th>\n",
       "      <th>FEATURE_1</th>\n",
       "      <th>FEATURE_2</th>\n",
       "      <th>FEATURE_3</th>\n",
       "      <th>FEATURE_4</th>\n",
       "      <th>FEATURE_5</th>\n",
       "      <th>FEATURE_6</th>\n",
       "      <th>FEATURE_7</th>\n",
       "      <th>FEATURE_8</th>\n",
       "      <th>FEATURE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>FEATURE_148_5</th>\n",
       "      <th>FEATURE_127_1</th>\n",
       "      <th>FEATURE_135_1</th>\n",
       "      <th>FEATURE_183_1</th>\n",
       "      <th>FEATURE_77_1</th>\n",
       "      <th>FEATURE_87_1</th>\n",
       "      <th>FEATURE_88_1</th>\n",
       "      <th>FEATURE_186_1</th>\n",
       "      <th>FEATURE_244_1</th>\n",
       "      <th>SUM_FEATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.027275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151109</td>\n",
       "      <td>0.029359</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056985</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058852</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255933</td>\n",
       "      <td>0.029359</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FEATURE_0  FEATURE_1  FEATURE_2  FEATURE_3  FEATURE_4  FEATURE_5  \\\n",
       "0   0.005195   0.051282        0.0        0.0        0.0        0.0   \n",
       "1   0.002597   0.025641        0.0        0.0        1.0        0.0   \n",
       "2   0.000000   0.000000        0.0        0.0        0.0        0.0   \n",
       "3   0.002597   0.025641        0.0        0.0        0.0        0.0   \n",
       "4   0.010390   0.076923        1.0        0.0        1.0        0.0   \n",
       "\n",
       "   FEATURE_6  FEATURE_7  FEATURE_8  FEATURE_9     ...       FEATURE_148_5  \\\n",
       "0        0.0   0.017228   0.027275   0.000000     ...                 0.0   \n",
       "1        0.0   0.151109   0.029359   0.272727     ...                 1.0   \n",
       "2        0.0   0.056985   0.028302   0.000000     ...                 0.0   \n",
       "3        0.0   0.058852   0.032526   0.000000     ...                 0.0   \n",
       "4        0.0   0.255933   0.029359   0.272727     ...                 0.0   \n",
       "\n",
       "   FEATURE_127_1  FEATURE_135_1  FEATURE_183_1  FEATURE_77_1  FEATURE_87_1  \\\n",
       "0            0.0            1.0            0.0           1.0           0.0   \n",
       "1            1.0            1.0            0.0           1.0           1.0   \n",
       "2            0.0            0.0            0.0           0.0           0.0   \n",
       "3            0.0            0.0            1.0           0.0           0.0   \n",
       "4            1.0            0.0            0.0           1.0           1.0   \n",
       "\n",
       "   FEATURE_88_1  FEATURE_186_1  FEATURE_244_1  SUM_FEATURE  \n",
       "0           0.0            0.0            1.0          0.0  \n",
       "1           1.0            1.0            1.0          1.0  \n",
       "2           0.0            1.0            1.0          0.0  \n",
       "3           0.0            1.0            0.0          0.0  \n",
       "4           1.0            0.0            1.0          0.0  \n",
       "\n",
       "[5 rows x 268 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/contest_test.csv')\n",
    "clean_test_data, test_ID = preprocess_test_data(test_data, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FEATURE_0</th>\n",
       "      <th>FEATURE_1</th>\n",
       "      <th>FEATURE_2</th>\n",
       "      <th>FEATURE_3</th>\n",
       "      <th>FEATURE_4</th>\n",
       "      <th>FEATURE_5</th>\n",
       "      <th>FEATURE_6</th>\n",
       "      <th>FEATURE_7</th>\n",
       "      <th>FEATURE_8</th>\n",
       "      <th>FEATURE_9</th>\n",
       "      <th>...</th>\n",
       "      <th>FEATURE_148_5</th>\n",
       "      <th>FEATURE_127_1</th>\n",
       "      <th>FEATURE_135_1</th>\n",
       "      <th>FEATURE_183_1</th>\n",
       "      <th>FEATURE_77_1</th>\n",
       "      <th>FEATURE_87_1</th>\n",
       "      <th>FEATURE_88_1</th>\n",
       "      <th>FEATURE_186_1</th>\n",
       "      <th>FEATURE_244_1</th>\n",
       "      <th>SUM_FEATURE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037474</td>\n",
       "      <td>0.024319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158135</td>\n",
       "      <td>0.029084</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.332850</td>\n",
       "      <td>0.344223</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003261</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050589</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044815</td>\n",
       "      <td>0.025210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 268 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FEATURE_0  FEATURE_1  FEATURE_2  FEATURE_3  FEATURE_4  FEATURE_5  \\\n",
       "0   0.000543   0.012821        0.0        0.0        0.0        0.0   \n",
       "1   0.000000   0.000000        1.0        0.0        0.0        0.0   \n",
       "2   0.000000   0.000000        0.0        0.0        1.0        0.0   \n",
       "3   0.003261   0.025641        0.0        0.0        0.0        0.0   \n",
       "4   0.000543   0.012821        0.0        0.0        0.0        0.0   \n",
       "\n",
       "   FEATURE_6  FEATURE_7  FEATURE_8  FEATURE_9     ...       FEATURE_148_5  \\\n",
       "0        0.0   0.037474   0.024319        0.0     ...                 0.0   \n",
       "1        0.0   0.158135   0.029084        0.2     ...                 0.0   \n",
       "2        0.0   0.332850   0.344223        0.2     ...                 0.0   \n",
       "3        0.0   0.050589   0.019639        0.0     ...                 0.0   \n",
       "4        0.0   0.044815   0.025210        0.0     ...                 0.0   \n",
       "\n",
       "   FEATURE_127_1  FEATURE_135_1  FEATURE_183_1  FEATURE_77_1  FEATURE_87_1  \\\n",
       "0            0.0            1.0            0.0           0.0           0.0   \n",
       "1            0.0            1.0            0.0           1.0           1.0   \n",
       "2            0.0            0.0            0.0           1.0           0.0   \n",
       "3            0.0            0.0            0.0           1.0           1.0   \n",
       "4            0.0            1.0            0.0           1.0           1.0   \n",
       "\n",
       "   FEATURE_88_1  FEATURE_186_1  FEATURE_244_1  SUM_FEATURE  \n",
       "0           0.0            1.0            1.0          0.0  \n",
       "1           1.0            0.0            1.0          0.0  \n",
       "2           1.0            0.0            0.0          0.0  \n",
       "3           1.0            1.0            1.0          0.0  \n",
       "4           1.0            0.0            1.0          0.0  \n",
       "\n",
       "[5 rows x 268 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1154270440\n",
       "1    1147987574\n",
       "2    1129622364\n",
       "3     619797496\n",
       "4    7391484886\n",
       "Name: ID, dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ID.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Основной Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые шаги ниже характерны либо только для этапа тестирования финальной модели, либо для этапа обучения. В этом случае будет явно указано, для какого этапа приведен шаг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Добавляем в чистые данные регрессионный признак, а также кластеризацию за 6 и 12 кластеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще говоря, задача напоминает регрессию, посколько данные классы имеют довольно строгое отношение порядка, например, по времени использования, количеству траффика и т.д. Поэтому возникла идея обучить на тренировочной выборке регрессор и добавить его предсказания в данные. Среди нескольких регрессоров (SGD, Linear, Ridge и пр.), лучший результат показал GradientBoostingRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для обучения:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(clean_data, target, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/home/sanityseeker/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "reg = GradientBoostingRegressor()\n",
    "reg.fit(X_train, y_train)\n",
    "regression_train = reg.predict(X_train.copy())\n",
    "regression_test = reg.predict(X_test.copy())\n",
    "X_train['REGRESSION'] = regression_train\n",
    "X_test['REGRESSION'] = regression_test\n",
    "six_clusters = KMeans(n_clusters=6, precompute_distances = True, n_jobs=-1)\n",
    "six_clusters.fit(X_train)\n",
    "twelve_clusters = KMeans(n_clusters=12, precompute_distances = True, n_jobs=-1)\n",
    "twelve_clusters.fit(X_train)\n",
    "cluster_twelve_train = twelve_clusters.predict(X_train.copy())\n",
    "cluster_twelve_test = twelve_clusters.predict(X_test.copy())\n",
    "cluster_six_train = six_clusters.predict(X_train.copy())\n",
    "cluster_six_test = six_clusters.predict(X_test.copy())\n",
    "X_train['CLUSTER_6'] = cluster_six_train\n",
    "X_test['CLUSTER_6'] = cluster_six_test\n",
    "X_train['CLUSTER_12'] = cluster_twelve_train\n",
    "X_test['CLUSTER_12'] = cluster_twelve_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для теста:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = clean_data.copy()\n",
    "test_data = clean_test_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor()\n",
    "reg.fit(train_data, target)\n",
    "regression_train = reg.predict(train_data.copy())\n",
    "regression_test = reg.predict(test_data.copy())\n",
    "train_data['REGRESSION'] = regression_train\n",
    "test_data['REGRESSION'] = regression_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Появилась идея добавить признак кластеризации, т.к. возможно с его помощью получится найти какую-то структуру в данных, если она есть. Kmeans -- unsupervised алгоритм, поэтому речи о каком-либо лике ответа быть не может. Экспериментально установлено, что кластеризация на большее число кластеров для алгоритма оказывается полезнее. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_clusters = KMeans(n_clusters=6, precompute_distances = True, n_jobs=-1)\n",
    "six_clusters.fit(train_data)\n",
    "twelve_clusters = KMeans(n_clusters=12, precompute_distances = True, n_jobs=-1)\n",
    "twelve_clusters.fit(train_data)\n",
    "cluster_twelve_train = twelve_clusters.predict(train_data.copy())\n",
    "cluster_twelve_test = twelve_clusters.predict(test_data.copy())\n",
    "cluster_six_train = six_clusters.predict(train_data.copy())\n",
    "cluster_six_test = six_clusters.predict(test_data.copy())\n",
    "train_data['CLUSTER_6'] = cluster_six_train\n",
    "test_data['CLUSTER_6'] = cluster_six_test\n",
    "train_data['CLUSTER_12'] = cluster_twelve_train\n",
    "test_data['CLUSTER_12'] = cluster_twelve_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Сэмплируем данные и обучаем xgboost с подобранными ранее параметрами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для обучения:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.9, gamma=0.3,\n",
    "       learning_rate=0.002, max_delta_step=0, max_depth=6,\n",
    "       min_child_weight=6, missing=None, n_estimators=800, n_jobs=1,\n",
    "       nthread=8, objective='multi:softprob', random_state=0,\n",
    "       reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=1, seed=27,\n",
    "       silent=True, subsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = SMOTE(kind='svm', m_neighbors=15, n_jobs=8).fit_sample(X_train, y_train)\n",
    "res_X = pd.DataFrame(X_resampled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для теста:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clf = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.9, gamma=0.3,\n",
    "       learning_rate=0.002, max_delta_step=0, max_depth=6,\n",
    "       min_child_weight=6, missing=None, n_estimators=800, n_jobs=1,\n",
    "       nthread=8, objective='multi:softprob', random_state=0,\n",
    "       reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=1, seed=27,\n",
    "       silent=True, subsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17372\n",
       "1     5650\n",
       "2     1499\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. в данных классы распределены очень неравномерно, есть смысл просэмплировать минорные классы. Резать мажоритарный класс не хочется, т.к. есть желание по-максимуму сохранить данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_resampled, y_resampled = SMOTE(kind='svm', m_neighbors=15, n_jobs=8).fit_sample(train_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_X = pd.DataFrame(X_resampled, columns=train_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Отбрасываем менее важные признаки.\n",
    "\n",
    "Получаем лучшие признаки на отложенной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_train_test_split(clf, X_train, X_test, y_train, y_test, best_features_num, metric='macro'):\n",
    "    best_train = X_train.copy()\n",
    "    best_test = X_test.copy()\n",
    "    \n",
    "    start = time.time()\n",
    "    clf.fit(best_train, y_train)\n",
    "    finish = time.time() - start\n",
    "    print(finish, 'seconds needed for boosting on whole train')\n",
    "    \n",
    "    imps = np.array(clf.feature_importances_)\n",
    "    features = np.arange(len(imps))\n",
    "    sorted_features = [x for _, x in sorted(zip(imps, features), key=lambda pair: pair[0])]\n",
    "    sorted_features = sorted_features[::-1]\n",
    "    \n",
    "    best_num = len(imps)\n",
    "    preds = clf.predict(best_test)\n",
    "    if metric == 'macro':\n",
    "        fscore = metrics.f1_score(y_test, preds, average='macro')\n",
    "    if metric == 'binary':\n",
    "        fscore = metrics.f1_score(y_test, preds)\n",
    "    best_fscore = fscore\n",
    "    print('MACRO F_1: {} \\nACCURACY: {}'.format(fscore,  metrics.accuracy_score(y_test, preds)))\n",
    "                                   \n",
    "    for features_num in best_features_num:\n",
    "        best_features = sorted_features[:features_num]\n",
    "        train = X_train.copy()\n",
    "        test = X_test.copy()\n",
    "        best_columns = np.array(X_train.columns)[best_features]\n",
    "        train = train.loc[:, best_columns]\n",
    "        test = test.loc[:, best_columns]\n",
    "\n",
    "        start = time.time()\n",
    "        clf.fit(train, y_train)\n",
    "        finish = time.time() - start\n",
    "\n",
    "        print('{} seconds needed for {} best features'.format(finish, features_num))\n",
    "        preds = clf.predict(test)\n",
    "        if metric == 'macro':\n",
    "            fscore = metrics.f1_score(y_test, preds, average='macro')\n",
    "        if metric == 'binary':\n",
    "            fscore = metrics.f1_score(y_test, preds)\n",
    "        if fscore > best_fscore:\n",
    "            best_fscore = fscore\n",
    "            best_train, best_test = train.copy(), test.copy()\n",
    "            best_num = features_num\n",
    "        print('MACRO F_1: {} \\nACCURACY: {}'.format(fscore,  metrics.accuracy_score(y_test, preds)))\n",
    "    return best_fscore, best_train, best_test, best_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для обучения:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368.2341229915619 seconds needed for boosting on whole train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F_1: 0.5281686664183706 \n",
      "ACCURACY: 0.6621814475025484\n",
      "343.66720390319824 seconds needed for 250 best features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F_1: 0.5277086476194753 \n",
      "ACCURACY: 0.6617737003058104\n",
      "310.5842673778534 seconds needed for 210 best features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F_1: 0.5265878269475303 \n",
      "ACCURACY: 0.6601427115188583\n",
      "285.47608947753906 seconds needed for 190 best features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F_1: 0.5273630206888623 \n",
      "ACCURACY: 0.6607543323139653\n",
      "245.9658784866333 seconds needed for 160 best features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F_1: 0.5269029597873013 \n",
      "ACCURACY: 0.6621814475025484\n",
      "167.02087688446045 seconds needed for 100 best features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F_1: 0.5293574365014486 \n",
      "ACCURACY: 0.6619775739041794\n",
      "117.54115748405457 seconds needed for 80 best features\n",
      "MACRO F_1: 0.5288425350658285 \n",
      "ACCURACY: 0.6623853211009174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "fscore, res_train, res_test, num = best_train_test_split(clf, res_X, X_test, y_resampled, y_test, [250, 210, 190, 160, 100, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_train.to_csv('best_columns.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is better to use 100 best features\n"
     ]
    }
   ],
   "source": [
    "print('it is better to use', num, 'best features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для теста:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = pd.read_csv('data/best_columns.csv')\n",
    "# columns.drop(columns.columns[0], axis=1, inplace=True)\n",
    "# columns = columns.drop('Unnamed 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['REGRESSION', 'FEATURE_196', 'FEATURE_142', 'FEATURE_238',\n",
    "       'FEATURE_174', 'FEATURE_185', 'FEATURE_180', 'FEATURE_248',\n",
    "       'FEATURE_74', 'FEATURE_153', 'FEATURE_203', 'FEATURE_219',\n",
    "       'FEATURE_226', 'FEATURE_1', 'FEATURE_55', 'FEATURE_110', 'FEATURE_250',\n",
    "       'FEATURE_258', 'FEATURE_170', 'FEATURE_197', 'DROPPED_SUM',\n",
    "       'FEATURE_227', 'FEATURE_220', 'FEATURE_24', 'FEATURE_121',\n",
    "       'FEATURE_171', 'FEATURE_120', 'FEATURE_115', 'FEATURE_122', 'FEATURE_0',\n",
    "       'FEATURE_111', 'FEATURE_172', 'FEATURE_169', 'FEATURE_165',\n",
    "       'FEATURE_63', 'FEATURE_58', 'FEATURE_186_1', 'FEATURE_198',\n",
    "       'FEATURE_135', 'FEATURE_86', 'FEATURE_77', 'FEATURE_232', 'FEATURE_69',\n",
    "       'FEATURE_199', 'FEATURE_85', 'FEATURE_34', 'FEATURE_81', 'FEATURE_82',\n",
    "       'FEATURE_178', 'FEATURE_118', 'FEATURE_91', 'FEATURE_114', 'FEATURE_99',\n",
    "       'FEATURE_228', 'FEATURE_26', 'FEATURE_89', 'FEATURE_177', 'FEATURE_155',\n",
    "       'FEATURE_76', 'FEATURE_179', 'FEATURE_243', 'FEATURE_224',\n",
    "       'FEATURE_154', 'FEATURE_213', 'FEATURE_148', 'FEATURE_57',\n",
    "       'FEATURE_107', 'FEATURE_215', 'FEATURE_211', 'SUM_FEATURE',\n",
    "       'FEATURE_46', 'FEATURE_108', 'FEATURE_151', 'FEATURE_244',\n",
    "       'FEATURE_254', 'FEATURE_8', 'FEATURE_116', 'FEATURE_257', 'FEATURE_167',\n",
    "       'FEATURE_127', 'FEATURE_96', 'FEATURE_88', 'FEATURE_45', 'FEATURE_21',\n",
    "       'FEATURE_218', 'FEATURE_133', 'FEATURE_130', 'FEATURE_59',\n",
    "       'FEATURE_252', 'FEATURE_112', 'FEATURE_182', 'FEATURE_47', 'FEATURE_71',\n",
    "       'FEATURE_221', 'FEATURE_38', 'FEATURE_48', 'FEATURE_186', 'FEATURE_7',\n",
    "       'FEATURE_204', 'FEATURE_126']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \n",
      "/home/sanityseeker/.local/lib/python3.6/site-packages/pandas/core/indexing.py:1367: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self._getitem_tuple(key)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.loc[:, cols]\n",
    "test_data = test_data.loc[:, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_X = res_X.loc[:, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) I'm baggin', baggin' yoooouuuuu...\n",
    "\n",
    "*...put your lovin' hand out, baby*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем вспомогательную функцию для создания ансамбля"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ensemble(clf, X_train, y_train, X_test, clf_num=10):\n",
    "    predictions = None\n",
    "    for i in range(clf_num):\n",
    "        clf.set_params(seed=i)\n",
    "        clf.fit(X_train, y_train)\n",
    "        if predictions is None:\n",
    "            predictions = clf.predict_proba(X_test)\n",
    "        else:\n",
    "            predictions += clf.predict_proba(X_test)\n",
    "        print('Done with {} iteration'.format(i))\n",
    "        \n",
    "    predictions /= clf_num\n",
    "    answers = np.argmax(predictions, axis=1)\n",
    "    return answers, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для обучения:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers, predictions = generate_ensemble(clf, y_resampled, res_test, clf_num=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Для теста:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.9, gamma=0.3, learning_rate=0.002,\n",
       "       max_delta_step=0, max_depth=6, min_child_weight=6, missing=None,\n",
       "       n_estimators=800, n_jobs=1, nthread=8, objective='multi:softprob',\n",
       "       random_state=0, reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=1,\n",
       "       seed=27, silent=True, subsample=0.5)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_clf.fit(res_X, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 0 iteration\n",
      "Done with 1 iteration\n",
      "Done with 2 iteration\n",
      "Done with 3 iteration\n",
      "Done with 4 iteration\n",
      "Done with 5 iteration\n"
     ]
    }
   ],
   "source": [
    "final_answers, final_predictions = generate_ensemble(final_clf, res_X, y_resampled, test_data, clf_num=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Оценка результатов (для этапа обучения)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_details(preds, y_test):\n",
    "    equals = preds == y_test.values\n",
    "    TP = np.zeros(3)\n",
    "    TN = np.zeros(3)\n",
    "    FP = np.zeros(3)\n",
    "    FN = np.zeros(3)\n",
    "    Pr = np.zeros(3)\n",
    "    Rec = np.zeros(3)\n",
    "    Lift = np.zeros(3)\n",
    "    \n",
    "    for i in range(3):\n",
    "        TP[i] = np.sum(equals & (preds == i))\n",
    "        TN[i] = np.sum(equals & (preds != i))\n",
    "        FN[i] = np.sum(~equals & (y_test.values == i))\n",
    "        FP[i] = np.sum(~equals & (preds == i))\n",
    "        Pr[i] = float(TP[i]) / (TP[i] + FP[i])\n",
    "        Rec[i] = float(TP[i]) / (TP[i] + FN[i])\n",
    "        Lift[i] = Pr[i] * len(y_test) / (TP[i] + FN[i])\n",
    "        print('Precision for {} class: {} '.format(i, Pr[i]))\n",
    "        print('Recall for {} class: {} '.format(i, Rec[i]))\n",
    "        print('Lift for {} class: {}\\n'.format(i, Lift[i]))\n",
    "        \n",
    "    Micro_pr = float(np.sum(TP)) / (np.sum(TP) + np.sum(FP))\n",
    "    Micro_rec = float(np.sum(TP)) / (np.sum(TP) + np.sum(FN))\n",
    "    Micro_f_score = 2 * Micro_pr * Micro_rec / (Micro_pr + Micro_rec)\n",
    "    print('Micro precision: {}'.format(Micro_pr))\n",
    "    print('Micro recall: {}'.format(Micro_rec))\n",
    "    print('Micro_f_score: {}\\n'.format(Micro_f_score))\n",
    "    \n",
    "    \n",
    "    Macro_pr = np.sum(Pr) / float(len(Pr))\n",
    "    Macro_rec = np.sum(Rec) / float(len(Rec))\n",
    "    Macro_f_score = 2 * Macro_pr * Macro_rec / (Macro_pr + Macro_rec)\n",
    "    print('Macro precision: {}'.format(Macro_pr))\n",
    "    print('Macro recall: {}'.format(Macro_rec))\n",
    "    print('Macro_f_score: {}'.format(Macro_f_score))\n",
    "    Macro_f_score_LIB = metrics.f1_score(y_test, preds, average='macro')\n",
    "    print('Macro_f_score_LIB: {}'.format(Macro_f_score_LIB))\n",
    "    \n",
    "    return Macro_f_score_LIB, Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 0 class: 0.8056463595839525 \n",
      "Recall for 0 class: 0.7721446881230418 \n",
      "Lift for 0 class: 1.1255184829847016\n",
      "\n",
      "Precision for 1 class: 0.3461928934010152 \n",
      "Recall for 1 class: 0.3119853613906679 \n",
      "Lift for 1 class: 1.553592078803275\n",
      "\n",
      "Precision for 2 class: 0.36396396396396397 \n",
      "Recall for 2 class: 0.6710963455149501 \n",
      "Lift for 2 class: 5.931040675226722\n",
      "\n",
      "Micro precision: 0.6634046890927625\n",
      "Micro recall: 0.6634046890927625\n",
      "Micro_f_score: 0.6634046890927625\n",
      "\n",
      "Macro precision: 0.5052677389829773\n",
      "Macro recall: 0.5850754650095532\n",
      "Macro_f_score: 0.5422508367224458\n",
      "Macro_f_score_LIB: 0.5295675526881614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5295675526881614, array([1.12551848, 1.55359208, 5.93104068]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.fit(res_train, y_resampled).predict(res_test)\n",
    "get_details(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 0 class: 0.8052412150089339 \n",
      "Recall for 0 class: 0.7701509541441185 \n",
      "Lift for 0 class: 1.1249524806661408\n",
      "\n",
      "Precision for 1 class: 0.34510595358224017 \n",
      "Recall for 1 class: 0.312900274473925 \n",
      "Lift for 1 class: 1.5487142747675098\n",
      "\n",
      "Precision for 2 class: 0.36330935251798563 \n",
      "Recall for 2 class: 0.6710963455149501 \n",
      "Lift for 2 class: 5.92037333588279\n",
      "\n",
      "Micro precision: 0.6621814475025484\n",
      "Micro recall: 0.6621814475025484\n",
      "Micro_f_score: 0.6621814475025484\n",
      "\n",
      "Macro precision: 0.5045521737030532\n",
      "Macro recall: 0.5847158580443311\n",
      "Macro_f_score: 0.5416842293657472\n",
      "Macro_f_score_LIB: 0.5289773859349673\n"
     ]
    }
   ],
   "source": [
    "final_macro_f_score, final_lift = get_details(answers, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что если обучаться, например, без сэмплирования данных, то мы получим accuracy около 0.73, но в этом будет довольно мало смысла. Ввиду несбалансированности данных, такой подход будет вести к очень высокому recall и precision для 0-го (доминирующего) класса, и к recall < 0.1 для первого, например. Просэмплировав **тренировочную** выборку, мы смогли в три раза поднять recall первого класса и сделать модель более информативной.\n",
    "\n",
    "Интересно то, что второй, самый малый, класс определяется достаточно неплохо и без сэмплинга (recall и precision около 0.5). Возможно, это связано с тем, что отличия между абонентами этой категории от нулевого класса гораздо выше, чем у первой.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Расчет результатов (для этапа тестирования)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32819775, 0.5375373 , 0.13426499],\n",
       "       [0.49851096, 0.39148906, 0.11000001],\n",
       "       [0.10202257, 0.17186111, 0.72611636],\n",
       "       ...,\n",
       "       [0.5963299 , 0.27531815, 0.12835196],\n",
       "       [0.19286217, 0.5279997 , 0.27913815],\n",
       "       [0.5153042 , 0.3715168 , 0.11317898]], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ids_one_show = [(i_d, _) for i_d, _ in sorted(zip(test_ID, final_predictions), key=lambda probs: probs[1][1])]\n",
    "sorted_ids_one = [i_d for i_d, _ in sorted(zip(test_ID, final_predictions), key=lambda probs: probs[1][1])]\n",
    "sorted_ids_one_show = sorted_ids_one_show[::-1]\n",
    "sorted_ids_one = sorted_ids_one[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ones = sorted_ids_one[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ids_two_show = [(i_d, _) for i_d, _ in sorted(zip(test_ID, final_predictions), key=lambda probs: probs[1][2])]\n",
    "sorted_ids_two = [i_d for i_d, _ in sorted(zip(test_ID, final_predictions), key=lambda probs: probs[1][2])]\n",
    "sorted_ids_two_show = sorted_ids_two_show[::-1]\n",
    "sorted_ids_two = sorted_ids_two[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_twos = sorted_ids_two[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx = best_ones + best_twos\n",
    "values = [1] * 500 + [2] * 200\n",
    "vals = [(i_d, value) for i_d, value in zip(indx, values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_segments = pd.DataFrame(data=vals, columns=['ID', 'TARGET'])\n",
    "contest_segments.to_csv('contest_segments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = [(i_d, label) for i_d, label in zip(test_ID, final_answers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "contest_answer = pd.DataFrame(data=ans, columns=['ID', 'TARGET'])\n",
    "contest_answer.to_csv('contest_answer.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: 2 classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В презентации мы упоминали про вторую модель, построенную на основе градиентного бустинга над решающими деревьями. Ниже приведена ее архитектура."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея в том, чтобы построить два классификатора по принципу ``i-ый классификатор показывает, что класс > i``, а потом сложить их ответы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_more_than_zero = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.8999999999999999, gamma=0.30000000000000004,\n",
    "       learning_rate=0.002, max_delta_step=0, max_depth=6,\n",
    "       min_child_weight=2, missing=None, n_estimators=700, n_jobs=1,\n",
    "       nthread=8, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=1, seed=27,\n",
    "       silent=True, subsample=0.5)\n",
    "\n",
    "clf_more_than_one = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.8999999999999999, gamma=0.30000000000000004,\n",
    "       learning_rate=0.002, max_delta_step=0, max_depth=6,\n",
    "       min_child_weight=2, missing=None, n_estimators=700, n_jobs=1,\n",
    "       nthread=8, objective='binary:logistic', random_state=0,\n",
    "       reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=1, seed=27,\n",
    "       silent=True, subsample=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_res_y_train = y_resampled.copy()\n",
    "zero_res_y_train[zero_res_y_train > 1] = 1\n",
    "zero_res_y_test = y_test.copy()\n",
    "zero_res_y_test[zero_res_y_test > 1] = 1\n",
    "\n",
    "one_res_y_train = y_resampled.copy()\n",
    "one_res_y_train[one_res_y_train == 1] = 0\n",
    "one_res_y_train[one_res_y_train == 2] = 1\n",
    "one_res_y_test = y_test.copy()\n",
    "one_res_y_test[one_res_y_test == 1] = 0\n",
    "one_res_y_test[one_res_y_test == 2] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_num = [120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154.57032418251038 seconds needed for boosting on whole train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F_1: 0.6493227163642572 \n",
      "ACCURACY: 0.7096839959225281\n",
      "82.49921655654907 seconds needed for 120 best features\n",
      "MACRO F_1: 0.6489218975882055 \n",
      "ACCURACY: 0.709887869520897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "fscore, best_zero_train, best_zero_test, num = best_train_test_split(clf_more_than_zero, res_X, X_test, zero_res_y_train, zero_res_y_test, best_features_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163.08265805244446 seconds needed for boosting on whole train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO F_1: 0.7114947562138312 \n",
      "ACCURACY: 0.9221202854230377\n",
      "77.74241137504578 seconds needed for 120 best features\n",
      "MACRO F_1: 0.7103332753969873 \n",
      "ACCURACY: 0.9211009174311927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "fscore, best_one_train, best_one_test, num = best_train_test_split(clf_more_than_one, res_X, X_test, one_res_y_train, one_res_y_test, best_features_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_0, probs_0 = generate_ensemble(clf_more_than_zero, best_zero_train, zero_res_y_train, best_zero_test, clf_num=4)\n",
    "preds_1, probs_1 = generate_ensemble(clf_more_than_one, best_one_train, one_res_y_train, best_one_test, clf_num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8999999999999999, gamma=0.30000000000000004,\n",
       "       learning_rate=0.002, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=2, missing=None, n_estimators=700, n_jobs=1,\n",
       "       nthread=8, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=1, seed=27,\n",
       "       silent=True, subsample=0.5)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_more_than_zero.fit(best_zero_train, zero_res_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.8999999999999999, gamma=0.30000000000000004,\n",
       "       learning_rate=0.002, max_delta_step=0, max_depth=6,\n",
       "       min_child_weight=2, missing=None, n_estimators=700, n_jobs=1,\n",
       "       nthread=8, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0.3, reg_lambda=0.5, scale_pos_weight=1, seed=27,\n",
       "       silent=True, subsample=0.5)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_more_than_one.fit(best_one_train, one_res_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/home/sanityseeker/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "self_preds = clf_more_than_zero.predict(best_zero_test) + clf_more_than_one.predict(best_one_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for 0 class: 0.8043161271507728 \n",
      "Recall for 0 class: 0.7855311876958132 \n",
      "Lift for 0 class: 1.123660097885087\n",
      "\n",
      "Precision for 1 class: 0.33113828786453436 \n",
      "Recall for 1 class: 0.3220494053064959 \n",
      "Lift for 1 class: 1.4860322982392873\n",
      "\n",
      "Precision for 2 class: 0.4019370460048426 \n",
      "Recall for 2 class: 0.5514950166112956 \n",
      "Lift for 2 class: 6.549837909148681\n",
      "\n",
      "Micro precision: 0.6678899082568808\n",
      "Micro recall: 0.6678899082568808\n",
      "Micro_f_score: 0.6678899082568808\n",
      "\n",
      "Macro precision: 0.5124638203400499\n",
      "Macro recall: 0.5530252032045349\n",
      "Macro_f_score: 0.5319724598113974\n",
      "Macro_f_score_LIB: 0.5287764289193103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5287764289193103, array([1.1236601 , 1.4860323 , 6.54983791]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_details(self_preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds_0 + preds_1\n",
    "get_details(preds, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
